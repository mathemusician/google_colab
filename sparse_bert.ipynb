{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "sparse_bert.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMEJjqL4efM2/TGYALjy99D",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mathemusician/google_colab/blob/main/sparse_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4Mv8mZZ6ucu",
        "outputId": "181a3ace-9da2-4245-c7e5-7706dcf41a40"
      },
      "source": [
        "!git clone https://github.com/neuralmagic/sparseml\n",
        "!cd sparseml/integrations/huggingface-transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'sparseml'...\n",
            "remote: Enumerating objects: 293590, done.\u001b[K\n",
            "remote: Counting objects: 100% (10435/10435), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3235/3235), done.\u001b[K\n",
            "remote: Total 293590 (delta 7029), reused 10284 (delta 6924), pack-reused 283155\u001b[K\n",
            "Receiving objects: 100% (293590/293590), 211.02 MiB | 23.24 MiB/s, done.\n",
            "Resolving deltas: 100% (241953/241953), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11zJPUQdw7BB",
        "outputId": "94701a6a-dc8c-457e-ce8c-7385bea860fc"
      },
      "source": [
        "# copied from https://colab.research.google.com/drive/1_xkARB35307P0-BTnqMy0flmYrfoYi5T#scrollTo=igwruhYxE_a7\n",
        "!pip uninstall -y torch torchvision torchaudio\n",
        "# May need to change in the future if Colab no longer uses CUDA 11.0\n",
        "!pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: torch 1.9.0+cu102\n",
            "Uninstalling torch-1.9.0+cu102:\n",
            "  Successfully uninstalled torch-1.9.0+cu102\n",
            "Found existing installation: torchvision 0.10.0+cu102\n",
            "Uninstalling torchvision-0.10.0+cu102:\n",
            "  Successfully uninstalled torchvision-0.10.0+cu102\n",
            "\u001b[33mWARNING: Skipping torchaudio as it is not installed.\u001b[0m\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.7.1+cu110\n",
            "  Downloading https://download.pytorch.org/whl/cu110/torch-1.7.1%2Bcu110-cp37-cp37m-linux_x86_64.whl (1156.8 MB)\n",
            "\u001b[K     |███████████████████████         | 834.1 MB 1.4 MB/s eta 0:03:43tcmalloc: large alloc 1147494400 bytes == 0x5565709aa000 @  0x7fb37233e615 0x5565379d602c 0x556537ab617a 0x5565379d8e4d 0x556537acac0d 0x556537a4d0d8 0x556537a47c35 0x5565379da73a 0x556537a4cf40 0x556537a47c35 0x5565379da73a 0x556537a4993b 0x556537acba56 0x556537a48fb3 0x556537acba56 0x556537a48fb3 0x556537acba56 0x556537a48fb3 0x5565379dab99 0x556537a1de79 0x5565379d97b2 0x556537a4ce65 0x556537a47c35 0x5565379da73a 0x556537a4993b 0x556537a47c35 0x5565379da73a 0x556537a48b0e 0x5565379da65a 0x556537a48d67 0x556537a47c35\n",
            "\u001b[K     |█████████████████████████████▏  | 1055.7 MB 1.5 MB/s eta 0:01:09tcmalloc: large alloc 1434370048 bytes == 0x5565b5000000 @  0x7fb37233e615 0x5565379d602c 0x556537ab617a 0x5565379d8e4d 0x556537acac0d 0x556537a4d0d8 0x556537a47c35 0x5565379da73a 0x556537a4cf40 0x556537a47c35 0x5565379da73a 0x556537a4993b 0x556537acba56 0x556537a48fb3 0x556537acba56 0x556537a48fb3 0x556537acba56 0x556537a48fb3 0x5565379dab99 0x556537a1de79 0x5565379d97b2 0x556537a4ce65 0x556537a47c35 0x5565379da73a 0x556537a4993b 0x556537a47c35 0x5565379da73a 0x556537a48b0e 0x5565379da65a 0x556537a48d67 0x556537a47c35\n",
            "\u001b[K     |████████████████████████████████| 1156.7 MB 1.3 MB/s eta 0:00:01tcmalloc: large alloc 1445945344 bytes == 0x55660a7ec000 @  0x7fb37233e615 0x5565379d602c 0x556537ab617a 0x5565379d8e4d 0x556537acac0d 0x556537a4d0d8 0x556537a47c35 0x5565379da73a 0x556537a48d67 0x556537a47c35 0x5565379da73a 0x556537a48d67 0x556537a47c35 0x5565379da73a 0x556537a48d67 0x556537a47c35 0x5565379da73a 0x556537a48d67 0x556537a47c35 0x5565379da73a 0x556537a48d67 0x5565379da65a 0x556537a48d67 0x556537a47c35 0x5565379da73a 0x556537a4993b 0x556537a47c35 0x5565379da73a 0x556537a4993b 0x556537a47c35 0x5565379dadd1\n",
            "\u001b[K     |████████████████████████████████| 1156.8 MB 15 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.8.2+cu110\n",
            "  Downloading https://download.pytorch.org/whl/cu110/torchvision-0.8.2%2Bcu110-cp37-cp37m-linux_x86_64.whl (12.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.9 MB 140 kB/s \n",
            "\u001b[?25hCollecting torchaudio==0.7.2\n",
            "  Downloading torchaudio-0.7.2-cp37-cp37m-manylinux1_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu110) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu110) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.2+cu110) (7.1.2)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.7.1+cu110 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.7.1+cu110 torchaudio-0.7.2 torchvision-0.8.2+cu110\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgBQ1owt0b5X",
        "outputId": "77dcb5f8-1146-40fd-fc86-569fa54a65d3"
      },
      "source": [
        "!git clone https://github.com/neuralmagic/transformers.git\n",
        "!cd /content/transformers; pip install -e .\n",
        "!pip install datasets\n",
        "!pip install sparseml[torch]\n",
        "!pip install wandb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 73310, done.\u001b[K\n",
            "remote: Total 73310 (delta 0), reused 0 (delta 0), pack-reused 73310\u001b[K\n",
            "Receiving objects: 100% (73310/73310), 56.21 MiB | 22.20 MiB/s, done.\n",
            "Resolving deltas: 100% (52175/52175), done.\n",
            "Obtaining file:///content/transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (4.62.0)\n",
            "Collecting huggingface-hub==0.0.8\n",
            "  Downloading huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (4.6.4)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 8.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (21.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 39.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.7.0.dev0) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.7.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.7.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0.dev0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0.dev0) (2021.5.30)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.7.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.7.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.7.0.dev0) (7.1.2)\n",
            "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Running setup.py develop for transformers\n",
            "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.7.0.dev0\n",
            "Collecting datasets\n",
            "  Downloading datasets-1.11.0-py3-none-any.whl (264 kB)\n",
            "\u001b[K     |████████████████████████████████| 264 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 34.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Collecting fsspec>=2021.05.0\n",
            "  Downloading fsspec-2021.8.1-py3-none-any.whl (119 kB)\n",
            "\u001b[K     |████████████████████████████████| 119 kB 44.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.8)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: tqdm>=4.42 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.6.4)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: xxhash, fsspec, datasets\n",
            "Successfully installed datasets-1.11.0 fsspec-2021.8.1 xxhash-2.0.2\n",
            "Collecting sparseml[torch]\n",
            "  Downloading sparseml-0.6.0-py3-none-any.whl (533 kB)\n",
            "\u001b[K     |████████████████████████████████| 533 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting onnx<1.8.0,>=1.5.0\n",
            "  Downloading onnx-1.7.0-cp37-cp37m-manylinux1_x86_64.whl (7.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4 MB 17.7 MB/s \n",
            "\u001b[?25hCollecting onnxruntime>=1.0.0\n",
            "  Downloading onnxruntime-1.8.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.5 MB 40.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.25.0 in /usr/local/lib/python3.7/dist-packages (from sparseml[torch]) (1.1.5)\n",
            "Collecting sparsezoo~=0.6.0\n",
            "  Downloading sparsezoo-0.6.0-py3-none-any.whl (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 8.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from sparseml[torch]) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from sparseml[torch]) (1.4.1)\n",
            "Collecting pyyaml>=5.0.0\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 38.6 MB/s \n",
            "\u001b[?25hCollecting pydantic>=1.0.0\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 32.6 MB/s \n",
            "\u001b[?25hCollecting toposort>=1.0\n",
            "  Downloading toposort-1.6-py2.py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from sparseml[torch]) (21.0)\n",
            "Requirement already satisfied: tqdm>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from sparseml[torch]) (4.62.0)\n",
            "Requirement already satisfied: progressbar2>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from sparseml[torch]) (3.38.0)\n",
            "Collecting merge-args>=0.1.0\n",
            "  Downloading merge_args-0.1.4-py2.py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: jupyter>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from sparseml[torch]) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from sparseml[torch]) (2.23.0)\n",
            "Requirement already satisfied: scikit-image>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from sparseml[torch]) (0.16.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from sparseml[torch]) (5.4.8)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from sparseml[torch]) (7.6.3)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from sparseml[torch]) (3.2.2)\n",
            "Requirement already satisfied: torch<1.8,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from sparseml[torch]) (1.7.1+cu110)\n",
            "Collecting tensorboardX>=1.0\n",
            "  Downloading tensorboardX-2.4-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[K     |████████████████████████████████| 124 kB 46.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=1.0 in /usr/local/lib/python3.7/dist-packages (from sparseml[torch]) (2.6.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->sparseml[torch]) (4.10.1)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->sparseml[torch]) (5.5.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->sparseml[torch]) (5.0.5)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->sparseml[torch]) (5.1.3)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->sparseml[torch]) (1.0.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->sparseml[torch]) (3.5.1)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->sparseml[torch]) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->sparseml[torch]) (5.3.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->sparseml[torch]) (0.8.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->sparseml[torch]) (57.4.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->sparseml[torch]) (0.7.5)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->sparseml[torch]) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->sparseml[torch]) (2.6.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->sparseml[torch]) (4.4.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->sparseml[torch]) (1.0.18)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0.0->sparseml[torch]) (5.2.0)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0.0->sparseml[torch]) (5.1.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0.0->sparseml[torch]) (5.3.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0.0->sparseml[torch]) (5.6.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->sparseml[torch]) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->sparseml[torch]) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->sparseml[torch]) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->sparseml[torch]) (1.3.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=3.0.0->sparseml[torch]) (1.15.0)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->sparseml[torch]) (2.6.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->sparseml[torch]) (4.7.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->sparseml[torch]) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.7/dist-packages (from onnx<1.8.0,>=1.5.0->sparseml[torch]) (3.7.4.3)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from onnx<1.8.0,>=1.5.0->sparseml[torch]) (3.17.3)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from onnxruntime>=1.0.0->sparseml[torch]) (1.12)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.0->sparseml[torch]) (2018.9)\n",
            "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from progressbar2>=3.0.0->sparseml[torch]) (2.5.6)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets>=7.0.0->sparseml[torch]) (0.2.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sparseml[torch]) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sparseml[torch]) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sparseml[torch]) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sparseml[torch]) (3.0.4)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15.0->sparseml[torch]) (1.1.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15.0->sparseml[torch]) (2.6.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15.0->sparseml[torch]) (2.4.1)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15.0->sparseml[torch]) (7.1.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.0->sparseml[torch]) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.0->sparseml[torch]) (0.12.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.0->sparseml[torch]) (0.6.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.0->sparseml[torch]) (1.39.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.0->sparseml[torch]) (0.4.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.0->sparseml[torch]) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.0->sparseml[torch]) (1.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.0->sparseml[torch]) (1.34.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.0->sparseml[torch]) (0.37.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.0->sparseml[torch]) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.0->sparseml[torch]) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.0->sparseml[torch]) (4.2.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.0->sparseml[torch]) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.0->sparseml[torch]) (4.6.4)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.0->sparseml[torch]) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.0->sparseml[torch]) (3.1.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0.0->sparseml[torch]) (2.11.3)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0.0->sparseml[torch]) (1.8.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0.0->sparseml[torch]) (0.11.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.0.0->sparseml[torch]) (22.2.1)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter>=1.0.0->sparseml[torch]) (0.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=1.0->sparseml[torch]) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter>=1.0.0->sparseml[torch]) (2.0.1)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0.0->sparseml[torch]) (0.7.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0.0->sparseml[torch]) (1.4.3)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0.0->sparseml[torch]) (0.5.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0.0->sparseml[torch]) (0.8.4)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0.0->sparseml[torch]) (0.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0.0->sparseml[torch]) (4.0.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter>=1.0.0->sparseml[torch]) (0.5.1)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter>=1.0.0->sparseml[torch]) (1.10.0)\n",
            "Installing collected packages: pyyaml, onnx, toposort, sparsezoo, pydantic, onnxruntime, merge-args, tensorboardX, sparseml\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed merge-args-0.1.4 onnx-1.7.0 onnxruntime-1.8.1 pydantic-1.8.2 pyyaml-5.4.1 sparseml-0.6.0 sparsezoo-0.6.0 tensorboardX-2.4 toposort-1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRrzGtg0AUnn"
      },
      "source": [
        "# Recipe\n",
        "\n",
        "I've changed the default recipe slightly so that the run would finish faster:\n",
        " - num_train_epochs: 1; default: 30\n",
        " - preprocessing_num_workers: 1; default: 6\n",
        " - pad_to_max_length: True; default: not included\n",
        " - max_train_samples 20; default: not included\n",
        " - max_eval_samples 20; default: not included\n",
        " - max_predict_samples 20; default: not included\n",
        "\n",
        "Obviously, this won't give you an impressive F1 score, but it does save you from waiting 173 hours. (944x speedup)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRC98ahI1U_O",
        "outputId": "bb253ecd-b2e4-4eab-ca24-9903fc64918e"
      },
      "source": [
        "!cd sparseml/integrations/huggingface-transformers\n",
        "!python transformers/examples/pytorch/question-answering/run_qa.py \\\n",
        "  --pad_to_max_length True \\\n",
        "  --model_name_or_path bert-base-uncased \\\n",
        "  --dataset_name squad \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_train_samples 20 \\\n",
        "  --max_eval_samples 20 \\\n",
        "  --max_predict_samples 20 \\\n",
        "  --evaluation_strategy epoch \\\n",
        "  --per_device_train_batch_size 16 \\\n",
        "  --learning_rate 5e-5 \\\n",
        "  --max_seq_length 384 \\\n",
        "  --doc_stride 128 \\\n",
        "  --output_dir MODELS_DIR/bert-base-12layers_prune80 \\\n",
        "  --cache_dir cache \\\n",
        "  --preprocessing_num_workers 1 \\\n",
        "  --fp16 \\\n",
        "  --num_train_epochs 1 \\\n",
        "  --recipe sparseml/integrations/huggingface-transformers/recipes/bert-base-12layers_prune80.md \\\n",
        "  --onnx_export_path MODELS_DIR/bert-base-12layers_prune80/onnx \\\n",
        "  --save_strategy epoch \\\n",
        "  --save_total_limit 2 \\\n",
        "  --report_to wandb"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-09-10 08:34:03 __main__     WARNING  Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
            "09/10/2021 08:34:03 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
            "2021-09-10 08:34:03 __main__     INFO     Training/evaluation parameters TrainingArguments(output_dir=MODELS_DIR/bert-base-12layers_prune80, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=IntervalStrategy.EPOCH, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Sep10_08-34-03_20cc82d127dc, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.EPOCH, save_steps=500, save_total_limit=2, no_cuda=False, seed=42, fp16=True, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=MODELS_DIR/bert-base-12layers_prune80, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['wandb'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, log_on_each_node=True, _n_gpu=1, mp_parameters=)\n",
            "09/10/2021 08:34:03 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=MODELS_DIR/bert-base-12layers_prune80, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=IntervalStrategy.EPOCH, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Sep10_08-34-03_20cc82d127dc, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.EPOCH, save_steps=500, save_total_limit=2, no_cuda=False, seed=42, fp16=True, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=MODELS_DIR/bert-base-12layers_prune80, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['wandb'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, log_on_each_node=True, _n_gpu=1, mp_parameters=)\n",
            "09/10/2021 08:34:03 - WARNING - datasets.builder -   Reusing dataset squad (cache/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n",
            "[INFO|configuration_utils.py:517] 2021-09-10 08:34:03,709 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at cache/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:553] 2021-09-10 08:34:03,711 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.7.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:517] 2021-09-10 08:34:03,832 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at cache/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:553] 2021-09-10 08:34:03,833 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.7.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-09-10 08:34:04,478 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at cache/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-09-10 08:34:04,478 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at cache/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-09-10 08:34:04,478 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-09-10 08:34:04,478 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-09-10 08:34:04,478 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at cache/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "[INFO|modeling_utils.py:1155] 2021-09-10 08:34:04,627 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at cache/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "[WARNING|modeling_utils.py:1331] 2021-09-10 08:34:06,547 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1342] 2021-09-10 08:34:06,548 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 1/1 [00:00<00:00, 19.19ba/s]\n",
            "100% 1/1 [00:00<00:00, 21.79ba/s]\n",
            "[INFO|trainer.py:415] 2021-09-10 08:34:09,494 >> Using amp fp16 backend\n",
            "[INFO|trainer.py:1148] 2021-09-10 08:34:09,622 >> ***** Running training *****\n",
            "[INFO|trainer.py:1149] 2021-09-10 08:34:09,622 >>   Num examples = 20\n",
            "[INFO|trainer.py:1150] 2021-09-10 08:34:09,622 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:1151] 2021-09-10 08:34:09,622 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1152] 2021-09-10 08:34:09,623 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1153] 2021-09-10 08:34:09,623 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1154] 2021-09-10 08:34:09,623 >>   Total optimization steps = 2\n",
            "[INFO|integrations.py:675] 2021-09-10 08:34:09,626 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjvkyleeclarin\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mMODELS_DIR/bert-base-12layers_prune80\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jvkyleeclarin/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jvkyleeclarin/huggingface/runs/1fcbxj8l\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210910_083409-1fcbxj8l\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "100% 2/2 [00:05<00:00,  2.48s/it][INFO|trainer.py:516] 2021-09-10 08:34:17,752 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
            "[INFO|trainer.py:2134] 2021-09-10 08:34:17,756 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2136] 2021-09-10 08:34:17,756 >>   Num examples = 20\n",
            "[INFO|trainer.py:2139] 2021-09-10 08:34:17,756 >>   Batch size = 8\n",
            "\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 67% 2/3 [00:00<00:00,  2.71it/s]\u001b[A\n",
            "100% 3/3 [00:01<00:00,  2.68it/s]\u001b[A\n",
            "\n",
            "09/10/2021 08:34:19 - INFO - utils_qa -   Post-processing 20 example predictions split into 20 features.\n",
            "100% 20/20 [00:00<00:00, 383.35it/s]\n",
            "09/10/2021 08:34:19 - INFO - utils_qa -   Saving predictions to MODELS_DIR/bert-base-12layers_prune80/eval_predictions.json.\n",
            "09/10/2021 08:34:19 - INFO - utils_qa -   Saving nbest_preds to MODELS_DIR/bert-base-12layers_prune80/eval_nbest_predictions.json.\n",
            "                                 \n",
            "\u001b[A{'exact_match': 0.0, 'f1': 6.000000000000001, 'epoch': 1.0}\n",
            "100% 2/2 [00:07<00:00,  2.48s/it]\n",
            "100% 3/3 [00:01<00:00,  2.68it/s]\u001b[A\n",
            "                                 \u001b[A[INFO|trainer.py:1888] 2021-09-10 08:34:19,696 >> Saving model checkpoint to MODELS_DIR/bert-base-12layers_prune80/checkpoint-2\n",
            "[INFO|configuration_utils.py:351] 2021-09-10 08:34:19,698 >> Configuration saved in MODELS_DIR/bert-base-12layers_prune80/checkpoint-2/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-09-10 08:34:21,141 >> Model weights saved in MODELS_DIR/bert-base-12layers_prune80/checkpoint-2/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-09-10 08:34:21,142 >> tokenizer config file saved in MODELS_DIR/bert-base-12layers_prune80/checkpoint-2/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-09-10 08:34:21,143 >> Special tokens file saved in MODELS_DIR/bert-base-12layers_prune80/checkpoint-2/special_tokens_map.json\n",
            "{'train_runtime': 15.1228, 'train_samples_per_second': 1.323, 'train_steps_per_second': 0.132, 'epoch': 1.0}\n",
            "[INFO|trainer.py:1348] 2021-09-10 08:34:24,745 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "100% 2/2 [00:12<00:00,  6.26s/it]\n",
            "[INFO|trainer.py:1888] 2021-09-10 08:34:24,748 >> Saving model checkpoint to MODELS_DIR/bert-base-12layers_prune80\n",
            "[INFO|configuration_utils.py:351] 2021-09-10 08:34:24,750 >> Configuration saved in MODELS_DIR/bert-base-12layers_prune80/config.json\n",
            "[INFO|modeling_utils.py:889] 2021-09-10 08:34:27,099 >> Model weights saved in MODELS_DIR/bert-base-12layers_prune80/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1924] 2021-09-10 08:34:27,100 >> tokenizer config file saved in MODELS_DIR/bert-base-12layers_prune80/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1930] 2021-09-10 08:34:27,100 >> Special tokens file saved in MODELS_DIR/bert-base-12layers_prune80/special_tokens_map.json\n",
            "[INFO|trainer_pt_utils.py:907] 2021-09-10 08:34:27,140 >> ***** train metrics *****\n",
            "[INFO|trainer_pt_utils.py:912] 2021-09-10 08:34:27,140 >>   epoch                    =        1.0\n",
            "[INFO|trainer_pt_utils.py:912] 2021-09-10 08:34:27,140 >>   train_runtime            = 0:00:15.12\n",
            "[INFO|trainer_pt_utils.py:912] 2021-09-10 08:34:27,141 >>   train_samples            =         20\n",
            "[INFO|trainer_pt_utils.py:912] 2021-09-10 08:34:27,141 >>   train_samples_per_second =      1.323\n",
            "[INFO|trainer_pt_utils.py:912] 2021-09-10 08:34:27,141 >>   train_steps_per_second   =      0.132\n",
            "2021-09-10 08:34:27 __main__     INFO     *** Evaluate ***\n",
            "09/10/2021 08:34:27 - INFO - __main__ -   *** Evaluate ***\n",
            "[INFO|trainer.py:516] 2021-09-10 08:34:27,143 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
            "[INFO|trainer.py:2134] 2021-09-10 08:34:27,147 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2136] 2021-09-10 08:34:27,147 >>   Num examples = 20\n",
            "[INFO|trainer.py:2139] 2021-09-10 08:34:27,147 >>   Batch size = 8\n",
            "100% 3/3 [00:01<00:00,  2.67it/s]\n",
            "09/10/2021 08:34:29 - INFO - utils_qa -   Post-processing 20 example predictions split into 20 features.\n",
            "  0% 0/20 [00:00<?, ?it/s]\u001b[A09/10/2021 08:34:29 - INFO - utils_qa -   Saving predictions to MODELS_DIR/bert-base-12layers_prune80/eval_predictions.json.\n",
            "100% 20/20 [00:00<00:00, 360.71it/s]\n",
            "09/10/2021 08:34:29 - INFO - utils_qa -   Saving nbest_preds to MODELS_DIR/bert-base-12layers_prune80/eval_nbest_predictions.json.\n",
            "100% 3/3 [00:01<00:00,  2.47it/s]\n",
            "[INFO|trainer_pt_utils.py:907] 2021-09-10 08:34:29,219 >> ***** eval metrics *****\n",
            "[INFO|trainer_pt_utils.py:912] 2021-09-10 08:34:29,219 >>   epoch        = 1.0\n",
            "[INFO|trainer_pt_utils.py:912] 2021-09-10 08:34:29,219 >>   eval_samples =  20\n",
            "[INFO|trainer_pt_utils.py:912] 2021-09-10 08:34:29,220 >>   exact_match  = 0.0\n",
            "[INFO|trainer_pt_utils.py:912] 2021-09-10 08:34:29,220 >>   f1           = 6.0\n",
            "2021-09-10 08:34:29 __main__     INFO     *** Export to ONNX ***\n",
            "09/10/2021 08:34:29 - INFO - __main__ -   *** Export to ONNX ***\n",
            "[INFO|trainer.py:516] 2021-09-10 08:34:29,221 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
            "/content/transformers/src/transformers/models/bert/modeling_bert.py:195: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n",
            "/content/transformers/src/transformers/modeling_utils.py:1968: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  input_tensor.shape[chunk_dim] == tensor_shape for input_tensor in input_tensors\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 545\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/wandb/run-20210910_083409-1fcbxj8l/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/wandb/run-20210910_083409-1fcbxj8l/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ParamPruning/bert.encoder.layer.0.attention.self.query.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        ParamPruning/bert.encoder.layer.0.attention.self.key.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ParamPruning/bert.encoder.layer.0.attention.self.value.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    ParamPruning/bert.encoder.layer.0.attention.output.dense.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        ParamPruning/bert.encoder.layer.0.intermediate.dense.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              ParamPruning/bert.encoder.layer.0.output.dense.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ParamPruning/bert.encoder.layer.1.attention.self.query.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        ParamPruning/bert.encoder.layer.1.attention.self.key.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ParamPruning/bert.encoder.layer.1.attention.self.value.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    ParamPruning/bert.encoder.layer.1.attention.output.dense.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        ParamPruning/bert.encoder.layer.1.intermediate.dense.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              ParamPruning/bert.encoder.layer.1.output.dense.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ParamPruning/bert.encoder.layer.2.attention.self.query.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        ParamPruning/bert.encoder.layer.2.attention.self.key.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ParamPruning/bert.encoder.layer.2.attention.self.value.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    ParamPruning/bert.encoder.layer.2.attention.output.dense.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        ParamPruning/bert.encoder.layer.2.intermediate.dense.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              ParamPruning/bert.encoder.layer.2.output.dense.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ParamPruning/bert.encoder.layer.3.attention.self.query.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        ParamPruning/bert.encoder.layer.3.attention.self.key.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ParamPruning/bert.encoder.layer.3.attention.self.value.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    ParamPruning/bert.encoder.layer.3.attention.output.dense.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        ParamPruning/bert.encoder.layer.3.intermediate.dense.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              ParamPruning/bert.encoder.layer.3.output.dense.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ParamPruning/bert.encoder.layer.4.attention.self.query.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        ParamPruning/bert.encoder.layer.4.attention.self.key.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ParamPruning/bert.encoder.layer.4.attention.self.value.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    ParamPruning/bert.encoder.layer.4.attention.output.dense.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        ParamPruning/bert.encoder.layer.4.intermediate.dense.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              ParamPruning/bert.encoder.layer.4.output.dense.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ParamPruning/bert.encoder.layer.5.attention.self.query.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        ParamPruning/bert.encoder.layer.5.attention.self.key.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ParamPruning/bert.encoder.layer.5.attention.self.value.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    ParamPruning/bert.encoder.layer.5.attention.output.dense.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        ParamPruning/bert.encoder.layer.5.intermediate.dense.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              ParamPruning/bert.encoder.layer.5.output.dense.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ParamPruning/bert.encoder.layer.6.attention.self.query.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        ParamPruning/bert.encoder.layer.6.attention.self.key.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ParamPruning/bert.encoder.layer.6.attention.self.value.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    ParamPruning/bert.encoder.layer.6.attention.output.dense.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        ParamPruning/bert.encoder.layer.6.intermediate.dense.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              ParamPruning/bert.encoder.layer.6.output.dense.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ParamPruning/bert.encoder.layer.7.attention.self.query.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        ParamPruning/bert.encoder.layer.7.attention.self.key.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ParamPruning/bert.encoder.layer.7.attention.self.value.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    ParamPruning/bert.encoder.layer.7.attention.output.dense.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        ParamPruning/bert.encoder.layer.7.intermediate.dense.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              ParamPruning/bert.encoder.layer.7.output.dense.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ParamPruning/bert.encoder.layer.8.attention.self.query.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        ParamPruning/bert.encoder.layer.8.attention.self.key.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ParamPruning/bert.encoder.layer.8.attention.self.value.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    ParamPruning/bert.encoder.layer.8.attention.output.dense.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        ParamPruning/bert.encoder.layer.8.intermediate.dense.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              ParamPruning/bert.encoder.layer.8.output.dense.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ParamPruning/bert.encoder.layer.9.attention.self.query.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        ParamPruning/bert.encoder.layer.9.attention.self.key.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ParamPruning/bert.encoder.layer.9.attention.self.value.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    ParamPruning/bert.encoder.layer.9.attention.output.dense.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        ParamPruning/bert.encoder.layer.9.intermediate.dense.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              ParamPruning/bert.encoder.layer.9.output.dense.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     ParamPruning/bert.encoder.layer.10.attention.self.query.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       ParamPruning/bert.encoder.layer.10.attention.self.key.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     ParamPruning/bert.encoder.layer.10.attention.self.value.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   ParamPruning/bert.encoder.layer.10.attention.output.dense.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       ParamPruning/bert.encoder.layer.10.intermediate.dense.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             ParamPruning/bert.encoder.layer.10.output.dense.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     ParamPruning/bert.encoder.layer.11.attention.self.query.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       ParamPruning/bert.encoder.layer.11.attention.self.key.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     ParamPruning/bert.encoder.layer.11.attention.self.value.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   ParamPruning/bert.encoder.layer.11.attention.output.dense.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       ParamPruning/bert.encoder.layer.11.intermediate.dense.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             ParamPruning/bert.encoder.layer.11.output.dense.weight 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                  train/exact_match 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                           train/f1 6.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                        train/epoch 1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                  train/global_step 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                           _runtime 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                         _timestamp 1631262869\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                              _step 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                train/train_runtime 15.1228\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                     train/train_samples_per_second 1.323\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                       train/train_steps_per_second 0.132\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                   train/total_flos 5017798010880.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ParamPruning/bert.encoder.layer.0.attention.self.query.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        ParamPruning/bert.encoder.layer.0.attention.self.key.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ParamPruning/bert.encoder.layer.0.attention.self.value.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    ParamPruning/bert.encoder.layer.0.attention.output.dense.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        ParamPruning/bert.encoder.layer.0.intermediate.dense.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              ParamPruning/bert.encoder.layer.0.output.dense.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ParamPruning/bert.encoder.layer.1.attention.self.query.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        ParamPruning/bert.encoder.layer.1.attention.self.key.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ParamPruning/bert.encoder.layer.1.attention.self.value.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    ParamPruning/bert.encoder.layer.1.attention.output.dense.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        ParamPruning/bert.encoder.layer.1.intermediate.dense.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              ParamPruning/bert.encoder.layer.1.output.dense.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ParamPruning/bert.encoder.layer.2.attention.self.query.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        ParamPruning/bert.encoder.layer.2.attention.self.key.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ParamPruning/bert.encoder.layer.2.attention.self.value.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    ParamPruning/bert.encoder.layer.2.attention.output.dense.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        ParamPruning/bert.encoder.layer.2.intermediate.dense.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              ParamPruning/bert.encoder.layer.2.output.dense.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ParamPruning/bert.encoder.layer.3.attention.self.query.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        ParamPruning/bert.encoder.layer.3.attention.self.key.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ParamPruning/bert.encoder.layer.3.attention.self.value.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    ParamPruning/bert.encoder.layer.3.attention.output.dense.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        ParamPruning/bert.encoder.layer.3.intermediate.dense.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              ParamPruning/bert.encoder.layer.3.output.dense.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ParamPruning/bert.encoder.layer.4.attention.self.query.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        ParamPruning/bert.encoder.layer.4.attention.self.key.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ParamPruning/bert.encoder.layer.4.attention.self.value.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    ParamPruning/bert.encoder.layer.4.attention.output.dense.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        ParamPruning/bert.encoder.layer.4.intermediate.dense.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              ParamPruning/bert.encoder.layer.4.output.dense.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ParamPruning/bert.encoder.layer.5.attention.self.query.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        ParamPruning/bert.encoder.layer.5.attention.self.key.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ParamPruning/bert.encoder.layer.5.attention.self.value.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    ParamPruning/bert.encoder.layer.5.attention.output.dense.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        ParamPruning/bert.encoder.layer.5.intermediate.dense.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              ParamPruning/bert.encoder.layer.5.output.dense.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ParamPruning/bert.encoder.layer.6.attention.self.query.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        ParamPruning/bert.encoder.layer.6.attention.self.key.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ParamPruning/bert.encoder.layer.6.attention.self.value.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    ParamPruning/bert.encoder.layer.6.attention.output.dense.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        ParamPruning/bert.encoder.layer.6.intermediate.dense.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              ParamPruning/bert.encoder.layer.6.output.dense.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ParamPruning/bert.encoder.layer.7.attention.self.query.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        ParamPruning/bert.encoder.layer.7.attention.self.key.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ParamPruning/bert.encoder.layer.7.attention.self.value.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    ParamPruning/bert.encoder.layer.7.attention.output.dense.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        ParamPruning/bert.encoder.layer.7.intermediate.dense.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              ParamPruning/bert.encoder.layer.7.output.dense.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ParamPruning/bert.encoder.layer.8.attention.self.query.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        ParamPruning/bert.encoder.layer.8.attention.self.key.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ParamPruning/bert.encoder.layer.8.attention.self.value.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    ParamPruning/bert.encoder.layer.8.attention.output.dense.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        ParamPruning/bert.encoder.layer.8.intermediate.dense.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              ParamPruning/bert.encoder.layer.8.output.dense.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ParamPruning/bert.encoder.layer.9.attention.self.query.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        ParamPruning/bert.encoder.layer.9.attention.self.key.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ParamPruning/bert.encoder.layer.9.attention.self.value.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    ParamPruning/bert.encoder.layer.9.attention.output.dense.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        ParamPruning/bert.encoder.layer.9.intermediate.dense.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              ParamPruning/bert.encoder.layer.9.output.dense.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     ParamPruning/bert.encoder.layer.10.attention.self.query.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       ParamPruning/bert.encoder.layer.10.attention.self.key.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     ParamPruning/bert.encoder.layer.10.attention.self.value.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   ParamPruning/bert.encoder.layer.10.attention.output.dense.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       ParamPruning/bert.encoder.layer.10.intermediate.dense.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             ParamPruning/bert.encoder.layer.10.output.dense.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     ParamPruning/bert.encoder.layer.11.attention.self.query.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       ParamPruning/bert.encoder.layer.11.attention.self.key.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     ParamPruning/bert.encoder.layer.11.attention.self.value.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   ParamPruning/bert.encoder.layer.11.attention.output.dense.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       ParamPruning/bert.encoder.layer.11.intermediate.dense.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             ParamPruning/bert.encoder.layer.11.output.dense.weight ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                  train/exact_match ▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                           train/f1 ▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                        train/epoch ▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                  train/global_step ▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                           _runtime ▁▅█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                         _timestamp ▁▅█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                              _step ▁▅█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                train/train_runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                     train/train_samples_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                       train/train_steps_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                   train/total_flos ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mMODELS_DIR/bert-base-12layers_prune80\u001b[0m: \u001b[34mhttps://wandb.ai/jvkyleeclarin/huggingface/runs/1fcbxj8l\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}